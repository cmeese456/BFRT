# -*- coding: utf-8 -*-
"""FedAvg Traffic Prediction Sim Preprocessed V1.1 - Final - 4 sensors.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uH5iEfg7CC1ugE-teBNkfUcPTorjvNxR

Part of the code is adopted from - https://github.com/xiaochus/TrafficFlowPrediction


"""2. Make sure Colab is using GPU"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

  
# update sklearn
!pip uninstall scikit-learn -y
!pip install -U scikit-learn




"""8. Cell to set up INPUT_LENGTH (used in both pretraining and real time FL).

"""

# MUST RUN THIS CELL TO SET UP INPUT_LENGTH
INPUT_LENGTH = 12

"""Optional - Cell to construct the pretrained model.

<font color='red'>Note - </font>

<font color='red'>1 - Specify the  `pretrain_config` in this cell, which includes the batch number.</font>

<font color='red'>2 - MUST SPECIFY `pre_train_percentage` to be non-0 for the pretrain function to run (Just ignore this cell or set pre_train_percentage = 0 to skip pretrain if not needed!).</font>


"""

# build pretrain data
pre_train_percentage = 0
pretrain_config = {"batch": 256, "epochs": 150}

# create log folder indicating by current running date and time
from datetime import datetime
date_time = datetime.now().strftime("%m%d%Y_%H%M%S")
log_files_folder_path = f"/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1.1/{date_time}_pretrain"

# build pretrain dataset and model
pretrained_model_file_path = None

if pre_train_percentage:
  os.mkdir(log_files_folder_path)
  # create log directories for pretraining
  pretrain_log_dir_path = f'{log_files_folder_path}/pretrain'
  os.makedirs(pretrain_log_dir_path, exist_ok=True)
  # save pretrain config to file
  with open(f'{pretrain_log_dir_path}/pretrain_config.txt', 'w') as config_file:
    config_file.write(f"pretrain_config: {repr(pretrain_config)} + \n")
    config_file.write(f"INPUT_LENGTH: {repr(INPUT_LENGTH)} + \n")
    config_file.write(f"pre_train_percentage: {repr(pre_train_percentage)} + \n")
  # build pretrain_dataset
  pretrain_dataset, post_pretrain_data_index = build_pretrain_dataset(pre_train_percentage, INPUT_LENGTH)
  # process data
  X_train, y_train = process_pretrain_data(pretrain_dataset)
  X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
  # build pretrain model
  model_to_pretrain = build_lstm([INPUT_LENGTH, 64, 64, 1])
  # begin training
  pretrained_model_file_path = pretrain_model(model_to_pretrain, X_train, y_train, log_files_folder_path, pretrain_config)

  import pickle
  post_pretrain_data_index_saved_path = f'{log_files_folder_path}/post_pretrain_data_index.pkl'
  with open(post_pretrain_data_index_saved_path, 'wb') as f:
      pickle.dump(post_pretrain_data_index, f)

"""9. Cell of the chain_predict function. Sliding INPUT_LENGTH data points per window."""

# code for chain prediction
def chain_predict(model, initial_prediction_set):

  initial_prediction_set = np.reshape(initial_prediction_set, (initial_prediction_set.shape[0], initial_prediction_set.shape[1], 1))
  chained_predictions = model.predict(initial_prediction_set)
  chained_predictions = chained_predictions.reshape(chained_predictions.shape + (1,))
  
  for start_index in range(1, INPUT_LENGTH):
    next_INPUT_LENGTH_minus_ONE_points_slice = initial_prediction_set[:, start_index: INPUT_LENGTH, :]
    next_INPUT_LENGTH_points = np.concatenate((next_INPUT_LENGTH_minus_ONE_points_slice, chained_predictions), axis=1)
    new_prediction = model.predict(next_INPUT_LENGTH_points)
    new_prediction = new_prediction.reshape(new_prediction.shape + (1,))
    chained_predictions = np.concatenate((chained_predictions, new_prediction), axis=1)
    
  return chained_predictions[-1::]

"""10. Main functions for learning and predictions

(1) Data input of the learning process for both chained predictions and 1-step look_ahead prediction (as a baseline to compare with the errors from chained predictions).

Round 1 - Train on 1\~12 to learn the 13th point, so training set 1\~13

Round 2 - Train on 1\~24, learn 13th, 14th ... 24th

Round 3 - Train on 13\~36, learn 25th, 26th ... 36th

(2) Data input of the chained prediction process

Round 1 - Test on 13\~24

Round 2 - Test on 25\~36

Round 3 - Test on 37\~38

(3) Data input of the 1-step look_ahead prediction process

Round 1 - Testset input 1\~24 to predict on 13\~24

Round 2 - Testset input 13\~36 to predict on 25\~36

Round 3 - Testset input 25\~48 to predict on 37\~48

<font color='red'>NOTE - Naive iterating over data. Will not deal with potential repeated or missing data. </font>
"""

config = {"batch": 1, "epochs": 20}
communication_rounds = 240 # 1 comm round = 1 day

import sys
import pickle

log_files_folder_path = f"/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1.1/{date_time}_run1_pretrain_8748_epoch20"

pretrained_model_file_path = None
# specify specific pretrained model, or run the optional pretrain cell and comment this line to use the just-trained pretrained model
pretrained_model_file_path = '/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1.1/08262021_181808/pretrain/pretrain.h5'
with open("/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1.1/08262021_181808/post_pretrain_data_index.pkl", 'rb') as f:
    post_pretrain_data_index = pickle.load(f)

os.makedirs(f'{log_files_folder_path}/globals', exist_ok=True)

# tf.compat.v1.disable_v2_behavior() # model trained in tf1
# begin main function

# train, FedAvg, Prediction (simulating real-time training)

# init baseline models
baseline_models = {}
for sensor_file in all_sensor_files:
  sensor_id = sensor_file.split('.')[0]
  # create log directories for this sensor
  this_sensor_dir_path = f'{log_files_folder_path}/{sensor_id}'
  this_sensor_h5_baseline_model_path = f'{this_sensor_dir_path}/h5/baseline'
  this_sensor_h5_local_model_path = f'{this_sensor_dir_path}/h5/local'
  os.makedirs(this_sensor_h5_baseline_model_path, exist_ok=True)
  os.makedirs(this_sensor_h5_local_model_path, exist_ok=True)
  
  # baseline_model = build_lstm([INPUT_LENGTH, 64, 64, 1])
  baseline_model = load_model(pretrained_model_file_path)
  model_file_path = f'{this_sensor_h5_baseline_model_path}/{sensor_id}_baseline.h5'
  baseline_model.save(model_file_path)
  baseline_models[sensor_file] = {}
  baseline_models[sensor_file]['model_file_path'] = model_file_path
  baseline_models[sensor_file]['this_sensor_dir_path'] = this_sensor_dir_path

# init global model
if pretrained_model_file_path:
  print("Starting FL with the pretrained model...")
  global_model = load_model(pretrained_model_file_path)
else:
  global_model = build_lstm([INPUT_LENGTH, 64, 64, 1])
  global_model.compile(loss="mse", optimizer="rmsprop", metrics=['mape'])
  global_model_file_path = f'{log_files_folder_path}/globals/h5'
  os.makedirs(global_model_file_path, exist_ok=True)
  global_model.save(f'{global_model_file_path}/comm_0.h5')

# init prediction records
sensor_predicts = {}
for sensor_file in all_sensor_files:
  sensor_predicts[sensor_file] = {}
  sensor_predicts[sensor_file]['baseline_chained'] = []
  sensor_predicts[sensor_file]['local_chained'] = []
  sensor_predicts[sensor_file]['global_chained'] = []
  sensor_predicts[sensor_file]['baseline_onestep'] = []
  sensor_predicts[sensor_file]['local_onestep'] = []
  sensor_predicts[sensor_file]['global_onestep'] = []
  sensor_predicts[sensor_file]['true'] = []

# init FedAvg vars
sample_size_each_communication_round = INPUT_LENGTH 

# use the first sensor data created_time as the standard to ease simulation data slicing
file_path = os.path.join(data_path, all_sensor_files[0])
created_time_column = pd.read_csv(file_path, encoding='utf-8').fillna(0)['created_time']
# if pre_train_percentage:
#   starting_data_index = post_pretrain_data_index[all_sensor_files[0]]
# else:
#   starting_data_index = 0
# even without pretrain, should use the same test set
starting_data_index = post_pretrain_data_index[all_sensor_files[0]]

# print("starting_data_index", starting_data_index)

for round in range(1, communication_rounds + 1):
  local_model_weights = []
  global_weights = global_model.get_weights()
  print(f"Simulating comm round {round}...")
  if round == 1:
    training_data_starting_index = starting_data_index
  else:
    training_data_starting_index = starting_data_index + (round - 2) * sample_size_each_communication_round
  starting_created_time = created_time_column.iloc[training_data_starting_index]
  X_train_records = {}
  X_test_records = {}
  for sensor_file in all_sensor_files:
    sensor_id = sensor_file.split('.')[0]
    
    ''' processing data '''
    # data file path
    file_path = os.path.join(data_path, sensor_file)
    # read data
    whole_data = pd.read_csv(file_path, encoding='utf-8').fillna(0)
    # get training data slicing indexes
    # try:
      # training_data_starting_index = whole_data[whole_data['created_time'] == starting_created_time].index[0]
      # change it to naively iterate csv dates
    # except:
    #   print(f"Data error - Sensor {sensor_id} does not have the row with created_time {starting_created_time}.")
    #   # sys.exit(0)
    #   continue
    if round == 1:
      training_data_ending_index = training_data_starting_index + sample_size_each_communication_round
    else:
      training_data_ending_index = training_data_starting_index + sample_size_each_communication_round * 2 - 1

    debug_text = f"DEBUG INFO: {sensor_id} now uses its own data starting at {training_data_starting_index} to {training_data_ending_index}\n"
    print(debug_text)
    
    # get test data slicing indexes
    # only use the labels of the test data to compare with chained (feed-forward) predictions
    if round == 1:
      chained_test_data_starting_index = training_data_ending_index
      onestep_test_data_starting_index = training_data_starting_index
    else:
      chained_test_data_starting_index = training_data_ending_index + 1
      onestep_test_data_starting_index = training_data_starting_index + sample_size_each_communication_round
    chained_test_data_ending_index = chained_test_data_starting_index + sample_size_each_communication_round - 1
    onestep_test_data_ending_index = onestep_test_data_starting_index + sample_size_each_communication_round * 2 - 1
    # slice training data
    train_data = whole_data[training_data_starting_index: training_data_ending_index + 1]
    # slice test data (the next sample_size_each_communication_round amount of data)
    chained_test_data = whole_data[chained_test_data_starting_index: chained_test_data_ending_index + 1]
    onestep_test_data = whole_data[onestep_test_data_starting_index: onestep_test_data_ending_index + 1]
    # process data
    # X_test won't be used in chained predictions
    X_train, y_train, _, y_test, scaler = process_data(train_data, chained_test_data, True)
    X_train, y_train, X_test_onestep, y_test_onestep, scaler = process_data(train_data, onestep_test_data, False)
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
    X_test_onestep = np.reshape(X_test_onestep, (X_test_onestep.shape[0], X_test_onestep.shape[1], 1))
    X_train_records[sensor_file] = X_train
    X_test_records[sensor_file] = X_test_onestep

    ''' begin training '''
    print(f"{sensor_id} now training on row {training_data_starting_index} to {training_data_ending_index}...")
    # train baseline model
    print(f"{sensor_id} training baseline model..")
    new_baseline_model_path = train_baseline_model(round, baseline_models[sensor_file]['model_file_path'], X_train, y_train, sensor_id, baseline_models[sensor_file]['this_sensor_dir_path'], config)
    # train local model
    print(f"{sensor_id} training local model..")
    new_local_model_path, new_local_model_weights = train_local_model(round, global_weights, X_train, y_train, sensor_id, baseline_models[sensor_file]['this_sensor_dir_path'], config)  
    # record local model
    local_model_weights.append(new_local_model_weights)

    ''' predictions '''
    print(f"{sensor_id} now predicting on 3 models...")
    baseline_model = load_model(new_baseline_model_path)
    local_model = load_model(new_local_model_path)
    ''' Onestep predictions '''
    # import pdb
    # pdb.set_trace()
    baseline_predicted = baseline_model.predict(X_test_onestep)
    baseline_predicted = scaler.inverse_transform(baseline_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    local_predicted = local_model.predict(X_test_onestep)
    local_predicted = scaler.inverse_transform(local_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    sensor_predicts[sensor_file]['baseline_onestep'].append((round,baseline_predicted))
    sensor_predicts[sensor_file]['local_onestep'].append((round,local_predicted))
    ''' chain predictions '''
    baseline_predicted = chain_predict(baseline_model, X_train)
    baseline_predicted = scaler.inverse_transform(baseline_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    local_predicted = chain_predict(local_model, X_train)
    local_predicted = scaler.inverse_transform(local_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    sensor_predicts[sensor_file]['baseline_chained'].append((round,baseline_predicted))
    sensor_predicts[sensor_file]['local_chained'].append((round,local_predicted))
    ''' true data '''
    y_test = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(1, -1)[0]
    sensor_predicts[sensor_file]['true'].append((round,y_test))
    
  
  ''' Simulate FedAvg '''
  global_weights = np.mean(local_model_weights, axis=0)
  global_model.set_weights(global_weights)
  global_model.save(f'{log_files_folder_path}/globals/round_{round}.h5')
  # Predict by global model
  for sensor_file, sensor_attrs in sensor_predicts.items():
    print(f"Simulating {sensor_file} FedAvg...")
    # try:
      # global_predicted = global_model.predict(X_train_records[sensor_file])
    ''' onestep prediction '''
    global_predicted = global_model.predict(X_test_records[sensor_file])
    global_predicted = scaler.inverse_transform(global_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    sensor_predicts[sensor_file]['global_onestep'].append((round,global_predicted))
    ''' chain prediction '''
    # print(X_train_records[sensor_file])
    global_predicted = chain_predict(global_model, X_train_records[sensor_file])
    # except:
    #   print(f'Data error - {sensor_file} does not contain data point {starting_created_time}.')
    #   # sys.exit(1)
    #   continue
    global_predicted = scaler.inverse_transform(global_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    # print("global_predicted.shape", global_predicted.shape)
    sensor_predicts[sensor_file]['global_chained'].append((round,global_predicted))
  
 
  predictions_record_saved_path = f'{log_files_folder_path}/all_predicts.pkl'
  with open(predictions_record_saved_path, 'wb') as f:
      pickle.dump(sensor_predicts, f)

config = {"batch": 1, "epochs": 20}
communication_rounds = 240 # 1 comm round = 1 day

import sys
import pickle

log_files_folder_path = f"/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1.1/{date_time}_run2_pretrain_8748_epoch20"

pretrained_model_file_path = None
# specify specific pretrained model, or run the optional pretrain cell and comment this line to use the just-trained pretrained model
pretrained_model_file_path = '/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1.1/08262021_181808/pretrain/pretrain.h5'
with open("/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1.1/08262021_181808/post_pretrain_data_index.pkl", 'rb') as f:
    post_pretrain_data_index = pickle.load(f)

os.makedirs(f'{log_files_folder_path}/globals', exist_ok=True)

from keras.models import load_model
# tf.compat.v1.disable_v2_behavior() # model trained in tf1
# begin main function

# train, FedAvg, Prediction (simulating real-time training)

# init baseline models
baseline_models = {}
for sensor_file in all_sensor_files:
  sensor_id = sensor_file.split('.')[0]
  # create log directories for this sensor
  this_sensor_dir_path = f'{log_files_folder_path}/{sensor_id}'
  this_sensor_h5_baseline_model_path = f'{this_sensor_dir_path}/h5/baseline'
  this_sensor_h5_local_model_path = f'{this_sensor_dir_path}/h5/local'
  os.makedirs(this_sensor_h5_baseline_model_path, exist_ok=True)
  os.makedirs(this_sensor_h5_local_model_path, exist_ok=True)
  
  # baseline_model = build_lstm([INPUT_LENGTH, 64, 64, 1])
  baseline_model = load_model(pretrained_model_file_path)
  model_file_path = f'{this_sensor_h5_baseline_model_path}/{sensor_id}_baseline.h5'
  baseline_model.save(model_file_path)
  baseline_models[sensor_file] = {}
  baseline_models[sensor_file]['model_file_path'] = model_file_path
  baseline_models[sensor_file]['this_sensor_dir_path'] = this_sensor_dir_path

# init global model
if pretrained_model_file_path:
  print("Starting FL with the pretrained model...")
  global_model = load_model(pretrained_model_file_path)
else:
  global_model = build_lstm([INPUT_LENGTH, 64, 64, 1])
  global_model.compile(loss="mse", optimizer="rmsprop", metrics=['mape'])
  global_model_file_path = f'{log_files_folder_path}/globals/h5'
  os.makedirs(global_model_file_path, exist_ok=True)
  global_model.save(f'{global_model_file_path}/comm_0.h5')

# init prediction records
sensor_predicts = {}
for sensor_file in all_sensor_files:
  sensor_predicts[sensor_file] = {}
  sensor_predicts[sensor_file]['baseline_chained'] = []
  sensor_predicts[sensor_file]['local_chained'] = []
  sensor_predicts[sensor_file]['global_chained'] = []
  sensor_predicts[sensor_file]['baseline_onestep'] = []
  sensor_predicts[sensor_file]['local_onestep'] = []
  sensor_predicts[sensor_file]['global_onestep'] = []
  sensor_predicts[sensor_file]['true'] = []

# init FedAvg vars
sample_size_each_communication_round = INPUT_LENGTH 

# use the first sensor data created_time as the standard to ease simulation data slicing
file_path = os.path.join(data_path, all_sensor_files[0])
created_time_column = pd.read_csv(file_path, encoding='utf-8').fillna(0)['created_time']
# if pre_train_percentage:
#   starting_data_index = post_pretrain_data_index[all_sensor_files[0]]
# else:
#   starting_data_index = 0
# even without pretrain, should use the same test set
starting_data_index = post_pretrain_data_index[all_sensor_files[0]]

# print("starting_data_index", starting_data_index)

for round in range(1, communication_rounds + 1):
  local_model_weights = []
  global_weights = global_model.get_weights()
  print(f"Simulating comm round {round}...")
  if round == 1:
    training_data_starting_index = starting_data_index
  else:
    training_data_starting_index = starting_data_index + (round - 2) * sample_size_each_communication_round
  starting_created_time = created_time_column.iloc[training_data_starting_index]
  X_train_records = {}
  X_test_records = {}
  for sensor_file in all_sensor_files:
    sensor_id = sensor_file.split('.')[0]
    
    ''' processing data '''
    # data file path
    file_path = os.path.join(data_path, sensor_file)
    # read data
    whole_data = pd.read_csv(file_path, encoding='utf-8').fillna(0)
    # get training data slicing indexes
    # try:
      # training_data_starting_index = whole_data[whole_data['created_time'] == starting_created_time].index[0]
      # change it to naively iterate csv dates
    # except:
    #   print(f"Data error - Sensor {sensor_id} does not have the row with created_time {starting_created_time}.")
    #   # sys.exit(0)
    #   continue
    if round == 1:
      training_data_ending_index = training_data_starting_index + sample_size_each_communication_round
    else:
      training_data_ending_index = training_data_starting_index + sample_size_each_communication_round * 2 - 1

    debug_text = f"DEBUG INFO: {sensor_id} now uses its own data starting at {training_data_starting_index} to {training_data_ending_index}\n"
    print(debug_text)
    
    # get test data slicing indexes
    # only use the labels of the test data to compare with chained (feed-forward) predictions
    if round == 1:
      chained_test_data_starting_index = training_data_ending_index
      onestep_test_data_starting_index = training_data_starting_index
    else:
      chained_test_data_starting_index = training_data_ending_index + 1
      onestep_test_data_starting_index = training_data_starting_index + sample_size_each_communication_round
    chained_test_data_ending_index = chained_test_data_starting_index + sample_size_each_communication_round - 1
    onestep_test_data_ending_index = onestep_test_data_starting_index + sample_size_each_communication_round * 2 - 1
    # slice training data
    train_data = whole_data[training_data_starting_index: training_data_ending_index + 1]
    # slice test data (the next sample_size_each_communication_round amount of data)
    chained_test_data = whole_data[chained_test_data_starting_index: chained_test_data_ending_index + 1]
    onestep_test_data = whole_data[onestep_test_data_starting_index: onestep_test_data_ending_index + 1]
    # process data
    # X_test won't be used in chained predictions
    X_train, y_train, _, y_test, scaler = process_data(train_data, chained_test_data, True)
    X_train, y_train, X_test_onestep, y_test_onestep, scaler = process_data(train_data, onestep_test_data, False)
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
    X_test_onestep = np.reshape(X_test_onestep, (X_test_onestep.shape[0], X_test_onestep.shape[1], 1))
    X_train_records[sensor_file] = X_train
    X_test_records[sensor_file] = X_test_onestep

    ''' begin training '''
    print(f"{sensor_id} now training on row {training_data_starting_index} to {training_data_ending_index}...")
    # train baseline model
    print(f"{sensor_id} training baseline model..")
    new_baseline_model_path = train_baseline_model(round, baseline_models[sensor_file]['model_file_path'], X_train, y_train, sensor_id, baseline_models[sensor_file]['this_sensor_dir_path'], config)
    # train local model
    print(f"{sensor_id} training local model..")
    new_local_model_path, new_local_model_weights = train_local_model(round, global_weights, X_train, y_train, sensor_id, baseline_models[sensor_file]['this_sensor_dir_path'], config)  
    # record local model
    local_model_weights.append(new_local_model_weights)

    ''' predictions '''
    print(f"{sensor_id} now predicting on 3 models...")
    baseline_model = load_model(new_baseline_model_path)
    local_model = load_model(new_local_model_path)
    ''' Onestep predictions '''
    # import pdb
    # pdb.set_trace()
    baseline_predicted = baseline_model.predict(X_test_onestep)
    baseline_predicted = scaler.inverse_transform(baseline_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    local_predicted = local_model.predict(X_test_onestep)
    local_predicted = scaler.inverse_transform(local_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    sensor_predicts[sensor_file]['baseline_onestep'].append((round,baseline_predicted))
    sensor_predicts[sensor_file]['local_onestep'].append((round,local_predicted))
    ''' chain predictions '''
    baseline_predicted = chain_predict(baseline_model, X_train)
    baseline_predicted = scaler.inverse_transform(baseline_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    local_predicted = chain_predict(local_model, X_train)
    local_predicted = scaler.inverse_transform(local_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    sensor_predicts[sensor_file]['baseline_chained'].append((round,baseline_predicted))
    sensor_predicts[sensor_file]['local_chained'].append((round,local_predicted))
    ''' true data '''
    y_test = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(1, -1)[0]
    sensor_predicts[sensor_file]['true'].append((round,y_test))
    
  
  ''' Simulate FedAvg '''
  global_weights = np.mean(local_model_weights, axis=0)
  global_model.set_weights(global_weights)
  global_model.save(f'{log_files_folder_path}/globals/round_{round}.h5')
  # Predict by global model
  for sensor_file, sensor_attrs in sensor_predicts.items():
    print(f"Simulating {sensor_file} FedAvg...")
    # try:
      # global_predicted = global_model.predict(X_train_records[sensor_file])
    ''' onestep prediction '''
    global_predicted = global_model.predict(X_test_records[sensor_file])
    global_predicted = scaler.inverse_transform(global_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    sensor_predicts[sensor_file]['global_onestep'].append((round,global_predicted))
    ''' chain prediction '''
    # print(X_train_records[sensor_file])
    global_predicted = chain_predict(global_model, X_train_records[sensor_file])
    # except:
    #   print(f'Data error - {sensor_file} does not contain data point {starting_created_time}.')
    #   # sys.exit(1)
    #   continue
    global_predicted = scaler.inverse_transform(global_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    # print("global_predicted.shape", global_predicted.shape)
    sensor_predicts[sensor_file]['global_chained'].append((round,global_predicted))
  
 
predictions_record_saved_path = f'{log_files_folder_path}/all_predicts.pkl'
with open(predictions_record_saved_path, 'wb') as f:
    pickle.dump(sensor_predicts, f)

config = {"batch": 1, "epochs":20}
communication_rounds = 240 # 1 comm round = 1 day

import sys
import pickle

log_files_folder_path = f"/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1.1/{date_time}_run3_pretrain_8748_epoch20"

pretrained_model_file_path = None
# specify specific pretrained model, or run the optional pretrain cell and comment this line to use the just-trained pretrained model
pretrained_model_file_path = '/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1.1/08262021_181808/pretrain/pretrain.h5'
with open("/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1.1/08262021_181808/post_pretrain_data_index.pkl", 'rb') as f:
    post_pretrain_data_index = pickle.load(f)

os.makedirs(f'{log_files_folder_path}/globals', exist_ok=True)

from keras.models import load_model
# tf.compat.v1.disable_v2_behavior() # model trained in tf1
# begin main function

# train, FedAvg, Prediction (simulating real-time training)

# init baseline models
baseline_models = {}
for sensor_file in all_sensor_files:
  sensor_id = sensor_file.split('.')[0]
  # create log directories for this sensor
  this_sensor_dir_path = f'{log_files_folder_path}/{sensor_id}'
  this_sensor_h5_baseline_model_path = f'{this_sensor_dir_path}/h5/baseline'
  this_sensor_h5_local_model_path = f'{this_sensor_dir_path}/h5/local'
  os.makedirs(this_sensor_h5_baseline_model_path, exist_ok=True)
  os.makedirs(this_sensor_h5_local_model_path, exist_ok=True)
  
  # baseline_model = build_lstm([INPUT_LENGTH, 64, 64, 1])
  baseline_model = load_model(pretrained_model_file_path)
  model_file_path = f'{this_sensor_h5_baseline_model_path}/{sensor_id}_baseline.h5'
  baseline_model.save(model_file_path)
  baseline_models[sensor_file] = {}
  baseline_models[sensor_file]['model_file_path'] = model_file_path
  baseline_models[sensor_file]['this_sensor_dir_path'] = this_sensor_dir_path

# init global model
if pretrained_model_file_path:
  print("Starting FL with the pretrained model...")
  global_model = load_model(pretrained_model_file_path)
else:
  global_model = build_lstm([INPUT_LENGTH, 64, 64, 1])
  global_model.compile(loss="mse", optimizer="rmsprop", metrics=['mape'])
  global_model_file_path = f'{log_files_folder_path}/globals/h5'
  os.makedirs(global_model_file_path, exist_ok=True)
  global_model.save(f'{global_model_file_path}/comm_0.h5')

# init prediction records
sensor_predicts = {}
for sensor_file in all_sensor_files:
  sensor_predicts[sensor_file] = {}
  sensor_predicts[sensor_file]['baseline_chained'] = []
  sensor_predicts[sensor_file]['local_chained'] = []
  sensor_predicts[sensor_file]['global_chained'] = []
  sensor_predicts[sensor_file]['baseline_onestep'] = []
  sensor_predicts[sensor_file]['local_onestep'] = []
  sensor_predicts[sensor_file]['global_onestep'] = []
  sensor_predicts[sensor_file]['true'] = []

# init FedAvg vars
sample_size_each_communication_round = INPUT_LENGTH 

# use the first sensor data created_time as the standard to ease simulation data slicing
file_path = os.path.join(data_path, all_sensor_files[0])
created_time_column = pd.read_csv(file_path, encoding='utf-8').fillna(0)['created_time']
# if pre_train_percentage:
#   starting_data_index = post_pretrain_data_index[all_sensor_files[0]]
# else:
#   starting_data_index = 0
# even without pretrain, should use the same test set
starting_data_index = post_pretrain_data_index[all_sensor_files[0]]

# print("starting_data_index", starting_data_index)

for round in range(1, communication_rounds + 1):
  local_model_weights = []
  global_weights = global_model.get_weights()
  print(f"Simulating comm round {round}...")
  if round == 1:
    training_data_starting_index = starting_data_index
  else:
    training_data_starting_index = starting_data_index + (round - 2) * sample_size_each_communication_round
  starting_created_time = created_time_column.iloc[training_data_starting_index]
  X_train_records = {}
  X_test_records = {}
  for sensor_file in all_sensor_files:
    sensor_id = sensor_file.split('.')[0]
    
    ''' processing data '''
    # data file path
    file_path = os.path.join(data_path, sensor_file)
    # read data
    whole_data = pd.read_csv(file_path, encoding='utf-8').fillna(0)
    # get training data slicing indexes
    # try:
      # training_data_starting_index = whole_data[whole_data['created_time'] == starting_created_time].index[0]
      # change it to naively iterate csv dates
    # except:
    #   print(f"Data error - Sensor {sensor_id} does not have the row with created_time {starting_created_time}.")
    #   # sys.exit(0)
    #   continue
    if round == 1:
      training_data_ending_index = training_data_starting_index + sample_size_each_communication_round
    else:
      training_data_ending_index = training_data_starting_index + sample_size_each_communication_round * 2 - 1

    debug_text = f"DEBUG INFO: {sensor_id} now uses its own data starting at {training_data_starting_index} to {training_data_ending_index}\n"
    print(debug_text)
    
    # get test data slicing indexes
    # only use the labels of the test data to compare with chained (feed-forward) predictions
    if round == 1:
      chained_test_data_starting_index = training_data_ending_index
      onestep_test_data_starting_index = training_data_starting_index
    else:
      chained_test_data_starting_index = training_data_ending_index + 1
      onestep_test_data_starting_index = training_data_starting_index + sample_size_each_communication_round
    chained_test_data_ending_index = chained_test_data_starting_index + sample_size_each_communication_round - 1
    onestep_test_data_ending_index = onestep_test_data_starting_index + sample_size_each_communication_round * 2 - 1
    # slice training data
    train_data = whole_data[training_data_starting_index: training_data_ending_index + 1]
    # slice test data (the next sample_size_each_communication_round amount of data)
    chained_test_data = whole_data[chained_test_data_starting_index: chained_test_data_ending_index + 1]
    onestep_test_data = whole_data[onestep_test_data_starting_index: onestep_test_data_ending_index + 1]
    # process data
    # X_test won't be used in chained predictions
    X_train, y_train, _, y_test, scaler = process_data(train_data, chained_test_data, True)
    X_train, y_train, X_test_onestep, y_test_onestep, scaler = process_data(train_data, onestep_test_data, False)
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
    X_test_onestep = np.reshape(X_test_onestep, (X_test_onestep.shape[0], X_test_onestep.shape[1], 1))
    X_train_records[sensor_file] = X_train
    X_test_records[sensor_file] = X_test_onestep

    ''' begin training '''
    print(f"{sensor_id} now training on row {training_data_starting_index} to {training_data_ending_index}...")
    # train baseline model
    print(f"{sensor_id} training baseline model..")
    new_baseline_model_path = train_baseline_model(round, baseline_models[sensor_file]['model_file_path'], X_train, y_train, sensor_id, baseline_models[sensor_file]['this_sensor_dir_path'], config)
    # train local model
    print(f"{sensor_id} training local model..")
    new_local_model_path, new_local_model_weights = train_local_model(round, global_weights, X_train, y_train, sensor_id, baseline_models[sensor_file]['this_sensor_dir_path'], config)  
    # record local model
    local_model_weights.append(new_local_model_weights)

    ''' predictions '''
    print(f"{sensor_id} now predicting on 3 models...")
    baseline_model = load_model(new_baseline_model_path)
    local_model = load_model(new_local_model_path)
    ''' Onestep predictions '''
    # import pdb
    # pdb.set_trace()
    baseline_predicted = baseline_model.predict(X_test_onestep)
    baseline_predicted = scaler.inverse_transform(baseline_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    local_predicted = local_model.predict(X_test_onestep)
    local_predicted = scaler.inverse_transform(local_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    sensor_predicts[sensor_file]['baseline_onestep'].append((round,baseline_predicted))
    sensor_predicts[sensor_file]['local_onestep'].append((round,local_predicted))
    ''' chain predictions '''
    baseline_predicted = chain_predict(baseline_model, X_train)
    baseline_predicted = scaler.inverse_transform(baseline_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    local_predicted = chain_predict(local_model, X_train)
    local_predicted = scaler.inverse_transform(local_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    sensor_predicts[sensor_file]['baseline_chained'].append((round,baseline_predicted))
    sensor_predicts[sensor_file]['local_chained'].append((round,local_predicted))
    ''' true data '''
    y_test = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(1, -1)[0]
    sensor_predicts[sensor_file]['true'].append((round,y_test))
    
  
  ''' Simulate FedAvg '''
  global_weights = np.mean(local_model_weights, axis=0)
  global_model.set_weights(global_weights)
  global_model.save(f'{log_files_folder_path}/globals/round_{round}.h5')
  # Predict by global model
  for sensor_file, sensor_attrs in sensor_predicts.items():
    print(f"Simulating {sensor_file} FedAvg...")
    # try:
      # global_predicted = global_model.predict(X_train_records[sensor_file])
    ''' onestep prediction '''
    global_predicted = global_model.predict(X_test_records[sensor_file])
    global_predicted = scaler.inverse_transform(global_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    sensor_predicts[sensor_file]['global_onestep'].append((round,global_predicted))
    ''' chain prediction '''
    # print(X_train_records[sensor_file])
    global_predicted = chain_predict(global_model, X_train_records[sensor_file])
    # except:
    #   print(f'Data error - {sensor_file} does not contain data point {starting_created_time}.')
    #   # sys.exit(1)
    #   continue
    global_predicted = scaler.inverse_transform(global_predicted.reshape(-1, 1)).reshape(1, -1)[0]
    # print("global_predicted.shape", global_predicted.shape)
    sensor_predicts[sensor_file]['global_chained'].append((round,global_predicted))
  
 
predictions_record_saved_path = f'{log_files_folder_path}/all_predicts.pkl'
with open(predictions_record_saved_path, 'wb') as f:
    pickle.dump(sensor_predicts, f)

"""11. Calculate errors for each look head for each model per sensor"""

def calculate_errors(sensor_predicts):
  prediction_errors = {} # prediction_errors[sensor][model][error_type][look_ahead: 1~INPUT_LENGTH] = [error values based on comm round]
  for sensor_file, models_attr in sensor_predicts.items():
      sensor_id = sensor_file.split('.')[0]
      prediction_errors[sensor_id] = {}

      for model, predicts in models_attr.items():
        if model != 'true':
          prediction_errors[sensor_id][model] = {}
          prediction_errors[sensor_id][model]['MAE'] = {}
          prediction_errors[sensor_id][model]['MSE'] = {}
          prediction_errors[sensor_id][model]['RMSE'] = {}
          prediction_errors[sensor_id][model]['MAPE'] = {}
          for look_ahead in range(1, INPUT_LENGTH + 1):
              prediction_errors[sensor_id][model]['MAE'][look_ahead] = []
              prediction_errors[sensor_id][model]['MSE'][look_ahead] = []
              prediction_errors[sensor_id][model]['RMSE'][look_ahead] = []
              prediction_errors[sensor_id][model]['MAPE'][look_ahead] = []

          for predict in predicts:
            round = predict[0]
            data = predict[1]
            true_data = models_attr['true'][round - 1][1]
            for _ in range(1, INPUT_LENGTH + 1): # or range(1, len(data) + 1)
              prediction_errors[sensor_id][model]['MAE'][_].append(get_MAE(true_data[:_], data[:_]))
              prediction_errors[sensor_id][model]['MSE'][_].append(get_MSE(true_data[:_], data[:_]))
              prediction_errors[sensor_id][model]['RMSE'][_].append(get_RMSE(true_data[:_], data[:_]))
              prediction_errors[sensor_id][model]['MAPE'][_].append(get_MAPE(true_data[:_], data[:_]))
  return prediction_errors

import pickle
# with open(predictions_record_saved_path, 'rb') as f:
with open("/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1.1/09092021_204440/all_predicts.pkl", 'rb') as f:
    sensor_predicts = pickle.load(f)
prediction_errors = calculate_errors(sensor_predicts)

# plot errors
communication_rounds = 120
import matplotlib.pyplot as plt
from statistics import mean

def plot_error_values(prediction_errors):
  for sensor_id, model_attrs in prediction_errors.items():
    for model, error_types in model_attrs.items():
      for error_type, look_aheads in error_types.items():
        fig, ax = plt.subplots()
        error_averages = {}
        for look_ahead, error_values in look_aheads.items():
          ax.plot(range(1, communication_rounds + 1), error_values, label=look_ahead)
          error_averages[look_ahead] = mean(error_values)
        plt.legend()
        plt.grid(True)
        plt.xlabel('Comm Round')
        plt.ylabel('Error')
        title = f'{sensor_id} - {model} - {error_type}'
        print(title)
        plt.title(title)
        fig.set_size_inches(68.5, 10.5)
        fig = plt.gcf()
        plt.show()
        # print(f"Error average values for {title}")
        # for look_ahead, error_average in error_averages.items():
        #   print(f"look_ahead = {look_ahead}, error_average = {error_average:.2f}")
        # print()
plot_error_values(prediction_errors)

# show errors
communication_rounds = 120
import matplotlib.pyplot as plt
from statistics import mean

def plot_error_values(prediction_errors):
  for sensor_id, model_attrs in prediction_errors.items():
    for model, error_types in model_attrs.items():
      for error_type, look_aheads in error_types.items():
        # fig, ax = plt.subplots()
        error_averages = {}
        for look_ahead, error_values in look_aheads.items():
          # ax.plot(range(1, communication_rounds + 1), error_values, label=look_ahead)
          error_averages[look_ahead] = mean(error_values)
        # plt.legend()
        # plt.grid(True)
        # plt.xlabel('Comm Round')
        # plt.ylabel('Error')
        title = f'{sensor_id} - {model} - {error_type}'
        # print(title)
        # plt.title(title)
        # fig.set_size_inches(68.5, 10.5)
        # fig = plt.gcf()
        # plt.show()
        print(f"Error average values for {title}")
        for look_ahead, error_average in error_averages.items():
          print(f"look_ahead = {look_ahead}, error_average = {error_average:.2f}")
        print()
plot_error_values(prediction_errors)

from tabulate import tabulate
import operator
import textwrap
import inspect

# plot and show errors
communication_rounds = 120
import matplotlib.pyplot as plt
from statistics import mean

# https://www.geeksforgeeks.org/highlight-the-negative-values-red-and-positive-values-black-in-pandas-dataframe/
# Define a function which 
# returns string for 
# applymap() method
def highlight_max(cell):
    if type(cell) != str and cell < 0 :
        return 'background: red; color:black'
    else:
        return 'background: black; color: white'


def report_error(prediction_errors):
  error_mean_records = {}
  for sensor_id, model_attrs in prediction_errors.items():
    error_mean_records[sensor_id] = {}
    for model, error_types in model_attrs.items():
      error_mean_records[sensor_id][model] = {}
      for error_type, look_aheads in error_types.items():
        error_mean_records[sensor_id][model][error_type] = []
        for look_ahead, error_values in look_aheads.items():
          error_mean_records[sensor_id][model][error_type].append(mean(error_values))
  error_reports = {}

  for sensor_id, model_attrs in error_mean_records.items():
    error_reports[sensor_id] = {}
    error_reports[sensor_id]['bo-bc'] = {}
    error_reports[sensor_id]['lo-lc'] = {}
    error_reports[sensor_id]['go-gc'] = {}

  for sensor_id, model_attrs in error_mean_records.items():

    error_reports[sensor_id]['bo-bc']['MAE'] =  map(operator.sub, error_mean_records[sensor_id]['baseline_onestep']['MAE'], error_mean_records[sensor_id]['baseline_chained']['MAE'])
    error_reports[sensor_id]['lo-lc']['MAE'] =  map(operator.sub, error_mean_records[sensor_id]['local_onestep']['MAE'], error_mean_records[sensor_id]['local_chained']['MAE'])
    error_reports[sensor_id]['go-gc']['MAE'] =  map(operator.sub, error_mean_records[sensor_id]['global_onestep']['MAE'], error_mean_records[sensor_id]['global_chained']['MAE'])

    error_reports[sensor_id]['bo-bc']['MSE'] =  map(operator.sub, error_mean_records[sensor_id]['baseline_onestep']['MSE'], error_mean_records[sensor_id]['baseline_chained']['MSE'])
    error_reports[sensor_id]['lo-lc']['MSE'] =  map(operator.sub, error_mean_records[sensor_id]['local_onestep']['MSE'], error_mean_records[sensor_id]['local_chained']['MSE'])
    error_reports[sensor_id]['go-gc']['MSE'] =  map(operator.sub, error_mean_records[sensor_id]['global_onestep']['MSE'], error_mean_records[sensor_id]['global_chained']['MSE'])

    error_reports[sensor_id]['bo-bc']['RMSE'] =  map(operator.sub, error_mean_records[sensor_id]['baseline_onestep']['RMSE'], error_mean_records[sensor_id]['baseline_chained']['RMSE'])
    error_reports[sensor_id]['lo-lc']['RMSE'] =  map(operator.sub, error_mean_records[sensor_id]['local_onestep']['RMSE'], error_mean_records[sensor_id]['local_chained']['RMSE'])
    error_reports[sensor_id]['go-gc']['RMSE'] =  map(operator.sub, error_mean_records[sensor_id]['global_onestep']['RMSE'], error_mean_records[sensor_id]['global_chained']['RMSE'])
    
    error_reports[sensor_id]['bo-bc']['MAPE'] =  map(operator.sub, error_mean_records[sensor_id]['baseline_onestep']['MAPE'], error_mean_records[sensor_id]['baseline_chained']['MAPE'])
    error_reports[sensor_id]['lo-lc']['MAPE'] =  map(operator.sub, error_mean_records[sensor_id]['local_onestep']['MAPE'], error_mean_records[sensor_id]['local_chained']['MAPE'])
    error_reports[sensor_id]['go-gc']['MAPE'] =  map(operator.sub, error_mean_records[sensor_id]['global_onestep']['MAPE'], error_mean_records[sensor_id]['global_chained']['MAPE'])

  return error_reports

error_reports = report_error(prediction_errors)

def print_error_diff(error_reports):
  msg = '''bo-bc: avg_error(baseline_onestep) - avg_error(baseline_chained)
  lo-lc: avg_error(local_onestep) - avg_error(local_chained)
  go-gc: avg_error(global_onestep) - avg_error(global_chained)
  The higher the value, the better the chained prediction performs.
  '''
  print(inspect.cleandoc(msg) + '\n')
  for sensor_id, error_difference_types in error_reports.items():
    print(f'For {sensor_id}')
    for error_difference_type, error_types in error_difference_types.items():
      table = [['AHEAD']]
      for _ in range(1, INPUT_LENGTH + 1):
        table[0].append(_)
      print(error_difference_type)
      row_iter = 1
      for error_type, error_values in error_types.items():
        table.append([])
        table[row_iter].append(error_type)
        table[row_iter].extend(error_values)
        row_iter += 1
      #print(tabulate(table) + '\n')
      import pandas as pd
      table = pd.DataFrame.from_records(table)
      # Using apply method of style 
      # attribute of Pandas DataFrame
      table = table.round(2)
      display(table.style.applymap(highlight_max))

print_error_diff(error_reports)

# plotting
import matplotlib as mpl
import matplotlib.pyplot as plt
# plot_dir_path = f'/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1/{date_time}/plots'
#temp
plot_dir_path = f'/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1.1/09072021_213129/plots'
import os
os.makedirs(plot_dir_path, exist_ok=True)

def plot_and_save(sensor_predicts):
    """Plot
    Plot the true data and predicted data.
    Plot the errors between true data and predicted data.

    # Arguments
        y_true: List/ndarray, ture data.
        y_pred: List/ndarray, predicted data.
    """
    for sensor_file, models_attr in sensor_predicts.items():
      sensor_id = sensor_file.split('.')[0]
      plot_data = {}

      for model, predicts in models_attr.items():

        plot_data[model] = {}
        plot_data[model]['x'] = []
        plot_data[model]['y'] = []
        
        for predict in predicts:
          round = predict[0]
          data = predict[1]
          plot_data[model]['x'].extend(range((round - 1) * INPUT_LENGTH + 1, round * INPUT_LENGTH + 1))
          plot_data[model]['y'].extend(data)

      
      fig = plt.figure()
      ax = fig.add_subplot(111)
      
      x = plot_data['true']['x']
      # customize x axis labels
      my_xticks = []
      for i in range(len(x)):
        if i % INPUT_LENGTH == 0:
          my_xticks.append(i//INPUT_LENGTH + 1)
        else:
          my_xticks.append('')

      plt.xticks(x, my_xticks)
      

      ax.plot(x, plot_data['true']['y'][:len(x)], label='True Data')
      ax.plot(x, plot_data['local']['y'][:len(x)], label='local')
      ax.plot(x, plot_data['global']['y'][:len(x)], label='global')
      ax.plot(x, plot_data['baseline']['y'][:len(x)], label='baseline')

      print(f"For {sensor_id}, true data has {len(x)} points,\n local has {len(plot_data['local']['y'])}, global has {len(plot_data['global']['y'])}, and baseline has {len(plot_data['baseline']['y'])}")

      plt.legend()
      plt.grid(True)
      plt.xlabel('Comm Round')
      plt.ylabel('Volume')
      plt.title(sensor_id)
      fig = plt.gcf()
      fig.set_size_inches(228.5, 10.5)

      plt.savefig(f'{plot_dir_path}/{sensor_id}.png', bbox_inches='tight', dpi=100)
      plt.show()

import pickle
# with open(predictions_record_saved_path, 'rb') as f:
with open("/content/drive/MyDrive/Traffic Prediction FedAvg Simulation/device_outputs_Preprocessed_V1.1/09072021_213129/all_predicts.pkl", 'rb') as f:
    sensor_predicts = pickle.load(f)
plot_and_save(sensor_predicts)